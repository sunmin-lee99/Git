[cs231n 3강 예습]


loss function : W값이 얼마나 안좋은지에 대해 정량화해주는 값
optimization : 정량화하는 과정

SVM loss (= hinge loss)
: 데이터에 둔감함
(왜냐하면, 정답 클래스가 다른 클래스보다 높냐 낮냐만 판단 가능)
- loss의 최솟값 = 0 (가장 좋은 값)
- loss의 최댓값 = 무한대

sj : 정답이 아닌 클래스 score
syi : 정답인 클래스 score
=> 정답 클래스가 정답이 아닌 클래스보다 saftey margin보다 크면 loss=0

sanity check : loss가 잘 나오는지 검사하는 것

squared hinge loss : 제곱을 이용하여 구하는 방법
(좋은지 나쁜지 빠르게 알 수 있는 방법)

overfitting (과적합) : 정확도 낮음

data loss : train 입장
regularization (규제) : test 입장

Softmas claaifier (= cross entropy) 
: 모든 스코어를 exp 취하고 그걸 다 더한 다음 원하는 클래스의 점수를 exp 취하여 나눠준 값
= 확률값
-> 여기서 원하는 정답 클래스에 -log를 취해 loss를 구해야함
: multinomial logistic regression -> exp값이 취해져있는 형태
: 데이터에 민감함
(왜냐하면 확률로 계산되기때문에 데이터가 좀만 변경되어도 확률에 확 영향을 미침)

random search : 랜덤하게 포인트를 찾는것

gradient descent (경사 하강법) : 경사를 따라 조금씩 내려가는 느낌으로 계산하는 것

numerical gradient (디버깅할때 쓰는 방법)
: 수식으로 하나하나 계산해서 사용하는 방법
- 단점 : 정확하지 않음, 느림
- 장점 : 간편함

analystic gradient (보통 쓰는 방법)
- 단점 : 에러가 잘 나올 수 있음
- 장점 : 정확함, 빠름

stochastic gradient descent (SGD)
: minibatch를 두고 데이터 개수를 잘라서 사용하는 방법





