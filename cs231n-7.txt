[cs231n 7강]

loss가 한 방향으론 빠르게, 다른 방향으로 천천히 변화하면 어떻게 될까? 
Gradient Descent의 역할은?
-> shallow dimension에서 매우 느리고 jitter 모양

step마다 지그재그로 왔다갔다 해서 매우 느린 속도로 학습이 진행됨

고차원에선 이런 문제가 많이 발생 

x : value of parameter, y : loss

local minima
SGD get will stuck == local minima, gradient is zero, 빠져나오지 못함
100 million dimension일 경우 모든 방향에서 loss가 증가함. 실제로 local minima는 매우 드물게 나타남
Saddle(안장) points엔 더 내려가야하는데 gradient 엄청 작은 값이라 정말 느리게 진행됨
Saddle points much more common in high dimension
100 million dimension에선 한 방향으론 loss가 증가하고, 다른 방향으론 loss가 내려감. 이런 경우는 정말 흔하게 나타남
near the saddle point도 slope가 평평해 gradient가 0에 가까워짐
local minima보다 saddle point가 더 문제가 됨
local minima는 non-convex에서 gradient 음수(극대점), convex에선 gradient가 양수
(극소점) vs saddle point는 방향에 따라 gradient가 다름

maintain velocity
add our gradient estimates to the velocity
원래 가던 방향으로 가고자하는 관성을 이용하는 방법
saddle point에서 gradient가 0이어도 velocity가 유지되서 saddle point를 지나갈 수 있음
minima로 가는데 필요한 step이 줄어듬
friction을 보통 0.9 또는 0.99로 줌
velocity = running mean of gradients

Momentum update
overcome some noise in our gradient estimate
현재 위치에서 gradient를 구한 후, 그 값에서 Velocity만큼 옮김

Nesterov Momentum (NAG, Nesterov Accelerated Gradient)
Velocity로 옮긴 후 gradient를 구함
convex optimization 관점에서 유용

RMSProp
sqaured gradient를 축적만 하지 않고 decay rate를 도입
decay rate = 0.9 or 0.99
강화학습에선 RMSProp을 많이 사용

Adam
almost use
velocity와 squared_gradient를 모두 사용
first moment : Momentum
second moment : AdaGrad/RMSProp(RMSProp에 더 근접한 듯! decay rate가 있음)
첫 second_moment는 0!(beta2는 0.9 또는 0.99)
첫 timestep은 very very large step
위 문제를 해결하기 위해 Bias correction을 추가
bias correction이 first, second_moment를 0으로 시작할 수 있도록 만들어줌
beta1 = 0.9, beta2 = 0.999, learning_rate = 1e-3 or 5e-4











